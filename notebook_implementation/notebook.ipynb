{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "from typing import TypedDict, List, Annotated, Literal, Union\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "import operator\n",
    "\n",
    "from langgraph.types import Command, Send\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "from prompts import *\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Literal, Dict, Any\n",
    "from enum import Enum\n",
    "\n",
    "import uuid\n",
    "\n",
    "from tavily import TavilyClient\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_llm(\n",
    "        provider: Literal[\"openai\", \"anthropic\", \"google\", \"ollama\"],\n",
    "        model: str,\n",
    "        temperature: float = 0.5,\n",
    "):\n",
    "    if provider == \"openai\":\n",
    "        return ChatOpenAI(model=model, temperature=temperature)\n",
    "    elif provider == \"anthropic\":\n",
    "        return ChatAnthropic(model=model, temperature=temperature)\n",
    "    elif provider == \"google\":\n",
    "        return ChatGoogleGenerativeAI(model=model, temperature=temperature)\n",
    "    elif provider == \"ollama\":\n",
    "        return ChatOllama(model=model, temperature=temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_llm(\n",
    "    provider=\"openai\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FieldType(str, Enum):\n",
    "    string = \"string\"\n",
    "    number = \"number\"\n",
    "    array = \"array\"\n",
    "    boolean = \"boolean\"\n",
    "\n",
    "class SchemaField(BaseModel):\n",
    "    key: str = Field(..., description=\"The unique identifier for the field\")\n",
    "    type: FieldType = Field(..., description=\"The data type of the field\")\n",
    "    description: str = Field(..., description=\"Some descriptive information for the field\")\n",
    "\n",
    "class DatasetSchema(BaseModel):\n",
    "    generated_schema: list[SchemaField]\n",
    "\n",
    "class DatasetRecords(BaseModel):\n",
    "    dataset: List[Dict[str, Any]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Section(BaseModel):\n",
    "    section_name: str = Field(..., description=\"The name of this section of the report without its number\")\n",
    "    sub_sections: List[str] = Field(..., description=\"Comprehensive descriptions of sub-sections, each combining the sub-section title and its bullet points into a fluid, natural-language description\")\n",
    "\n",
    "class Sections(BaseModel):\n",
    "    sections: List[Section] = Field(..., description=\"A list of sections\")\n",
    "\n",
    "class Query(BaseModel):\n",
    "    query: str = Field(..., description=\"A search query\")\n",
    "\n",
    "class Queries(BaseModel):\n",
    "    queries: List[Query] = Field(..., description=\"A list of search queries\")\n",
    "\n",
    "class SearchResult(BaseModel):\n",
    "    query: Query = Field(..., description=\"The search query that was used to retrieve the raw content\")\n",
    "    raw_content: list[str] = Field(..., description=\"The raw content retrieved from the search\")\n",
    "\n",
    "class Feedback(BaseModel):\n",
    "    feedback: Union[str, bool] = Field(..., description=\"Feedback on the report structure. If the content is good for the section, return True (boolean), otherwise return a string of feedback on what is missing or incorrect.\")\n",
    "\n",
    "class SectionOutput(BaseModel):\n",
    "    # final_section_content: List[str] = Field(..., description=\"The final section content\")\n",
    "    final_section_dataset: List[Dict[str, Any]] = Field(..., description=\"The final section dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    topic: str\n",
    "    outline: str\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "    report_structure: str\n",
    "    sections: List[Section]\n",
    "    final_section_dataset: Annotated[List[Dict[str, Any]], operator.add] = []\n",
    "    final_dataset: List[Dict[str, Any]]\n",
    "    schema: DatasetSchema\n",
    "\n",
    "class ResearchState(TypedDict):\n",
    "    topic: str\n",
    "    report_structure: str\n",
    "    section: Section\n",
    "    knowledge: str\n",
    "    reflection_feedback: Feedback = Feedback(feedback=\"\")\n",
    "    generated_queries: List[Query] = []\n",
    "    searched_queries: Annotated[List[Query], operator.add] = []\n",
    "    search_results: Annotated[List[SearchResult], operator.add] = []\n",
    "    accumulated_content: str = \"\"\n",
    "    reflection_count: int = 1\n",
    "    final_section_content: List[str] = []\n",
    "    schema: DatasetSchema\n",
    "    final_section_dataset: List[Dict[str, Any]] = []\n",
    "    error: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_schema_generator_system_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(SCHEMA_GENERATION_PROMPT),\n",
    "    HumanMessagePromptTemplate.from_template(\n",
    "        template=\"\"\"\n",
    "        Topic: {topic}\n",
    "        Outline: {outline}\n",
    "        \"\"\"\n",
    "    ),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "])\n",
    "\n",
    "llm_with_schema_tool = llm.bind_tools(tools=[DatasetSchema], tool_choice=\"required\")\n",
    "schema_generator_llm = dataset_schema_generator_system_prompt | llm_with_schema_tool\n",
    "\n",
    "def schema_generator_node(state: AgentState, config: RunnableConfig):\n",
    "    result = schema_generator_llm.invoke(state)\n",
    "    suggested_schema = DatasetSchema.model_validate(result.tool_calls[0][\"args\"])\n",
    "\n",
    "    return {\"schema\": suggested_schema, \"messages\": f\"{[suggested_schema.generated_schema]}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_feedback_on_schema_node(state: AgentState, config: RunnableConfig) -> Command[Literal[\"report_structure_planner\", \"schema_generator\"]]:\n",
    "    human_message = input(\"Please provide feedback on the report structure (type 'continue' to continue): \")\n",
    "    schema = state.get(\"schema\")\n",
    "    if human_message == \"continue\":\n",
    "        return Command(\n",
    "            goto=\"report_structure_planner\",\n",
    "            update={\"messages\": [HumanMessage(content=human_message)], \"schema\": schema}\n",
    "        )\n",
    "    else:\n",
    "        return Command(\n",
    "            goto=\"schema_generator\",\n",
    "            update={\"messages\": [HumanMessage(content=human_message)]}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_structure_planner_system_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(REPORT_STRUCTURE_PLANNER_SYSTEM_PROMPT_TEMPLATE),\n",
    "    HumanMessagePromptTemplate.from_template(\n",
    "        template=\"\"\"\n",
    "        Topic: {topic}\n",
    "        Outline: {outline}\n",
    "        \"\"\"\n",
    "    ),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "])\n",
    "\n",
    "report_structure_planner_llm = report_structure_planner_system_prompt | llm\n",
    "\n",
    "def report_structure_planner_node(state: AgentState, config: RunnableConfig):\n",
    "    result = report_structure_planner_llm.invoke(state)\n",
    "    return {\"messages\": [result]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_feedback_node(state: AgentState, config: RunnableConfig)->Command[Literal[\"section_formatter\", \"report_structure_planner\"]]:\n",
    "    human_message = input(\"Please provide feedback on the report structure (type 'continue' to continue): \")\n",
    "    report_structure = state.get(\"messages\")[-1].content\n",
    "    if human_message == \"continue\":\n",
    "        return Command(\n",
    "            goto=\"section_formatter\",\n",
    "            update={\"messages\": [HumanMessage(content=human_message)], \"report_structure\": report_structure}\n",
    "        )\n",
    "    else:\n",
    "        return Command(\n",
    "            goto=\"report_structure_planner\",\n",
    "            update={\"messages\": [HumanMessage(content=human_message)]}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "section_formatter_system_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(SECTION_FORMATTER_SYSTEM_PROMPT_TEMPLATE),\n",
    "    HumanMessagePromptTemplate.from_template(template=\"{report_structure}\"),\n",
    "])\n",
    "\n",
    "section_formatter_llm = section_formatter_system_prompt | llm.with_structured_output(Sections)\n",
    "\n",
    "def section_formatter_node(state: AgentState, config: RunnableConfig) -> Command[Literal[\"research_agent\"]]:\n",
    "    result = section_formatter_llm.invoke(state)\n",
    "    schema = state.get(\"schema\")\n",
    "    report_structure = state.get(\"report_structure\")\n",
    "    topic = state.get(\"topic\")\n",
    "    # return {\"sections\": result.sections}\n",
    "    return Command(\n",
    "        update={\"sections\": result.sections},\n",
    "        goto=[\n",
    "            Send(\n",
    "                \"research_agent\",\n",
    "                {\n",
    "                    \"topic\": topic,\n",
    "                    \"section\": s,\n",
    "                    \"schema\": schema,\n",
    "                    \"report_structure\": report_structure,\n",
    "                }\n",
    "            ) for s in result.sections\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_knowledge_system_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(SECTION_KNOWLEDGE_SYSTEM_PROMPT_TEMPLATE),\n",
    "    HumanMessagePromptTemplate.from_template(template=\"{section}\"),\n",
    "])\n",
    "\n",
    "section_knowledge_llm = section_knowledge_system_prompt | llm\n",
    "\n",
    "def section_knowledge_node(state: ResearchState, config: RunnableConfig):\n",
    "    result = section_knowledge_llm.invoke(state)\n",
    "    return {\"knowledge\": result.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def query_generator_node(state: ResearchState, config: RunnableConfig):\n",
    "    query_generator_system_prompt = ChatPromptTemplate.from_messages([\n",
    "        SystemMessagePromptTemplate.from_template(QUERY_GENERATOR_SYSTEM_PROMPT_TEMPLATE),\n",
    "        HumanMessagePromptTemplate.from_template(template=\"Section: {section}\\nPrevious Queries: {searched_queries}\\nReflection Feedback: {reflection_feedback}\"),\n",
    "    ])\n",
    "\n",
    "    query_generator_llm = query_generator_system_prompt | llm.with_structured_output(Queries)\n",
    "    state.setdefault(\"reflection_feedback\", \"\")\n",
    "    state.setdefault(\"searched_queries\", [])\n",
    "    configurable = config.get(\"configurable\")\n",
    "\n",
    "    input_data = {\n",
    "        **state,\n",
    "        **configurable  # includes max_queries, search_depth, etc.\n",
    "    }\n",
    "\n",
    "    result = query_generator_llm.invoke(input_data, configurable)\n",
    "    return {\"generated_queries\": result.queries, \"searched_queries\": result.queries}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tavily_client = TavilyClient()\n",
    "\n",
    "def tavily_search_node(state: ResearchState, config: RunnableConfig):\n",
    "    queries = state[\"generated_queries\"]\n",
    "    configurable = config.get(\"configurable\")\n",
    "    search_results = []\n",
    "    for query in queries:\n",
    "        raw_content = []\n",
    "        response = tavily_client.search(query=query.query, max_results=configurable.get(\"search_depth\"), include_raw_content=True)\n",
    "        for result in response[\"results\"]:\n",
    "            raw_content.append(result['content'])\n",
    "        search_results.append(SearchResult(query=query, raw_content=raw_content))\n",
    "    return {\"search_results\": search_results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def result_accumulator_node(state: ResearchState, config: RunnableConfig):\n",
    "    result_accumulator_system_prompt = ChatPromptTemplate.from_messages([\n",
    "        SystemMessagePromptTemplate.from_template(RESULT_ACCUMULATOR_SYSTEM_PROMPT_TEMPLATE),\n",
    "        HumanMessagePromptTemplate.from_template(template=\"{search_results}\"),\n",
    "    ])\n",
    "\n",
    "    result_accumulator_llm = result_accumulator_system_prompt | llm\n",
    "    result = result_accumulator_llm.invoke(state)\n",
    "    return {\"accumulated_content\": result.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_feedback_system_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(REFLECTION_FEEDBACK_SYSTEM_PROMPT_TEMPLATE),\n",
    "    HumanMessagePromptTemplate.from_template(template=\"Section: {section}\\nAccumulated Content: {accumulated_content}\"),\n",
    "])\n",
    "\n",
    "reflection_feedback_llm = reflection_feedback_system_prompt | llm.with_structured_output(Feedback)\n",
    "\n",
    "def reflection_feedback_node(state: ResearchState, config: RunnableConfig) -> Command[Literal[\"final_section_formatter\", \"query_generator\"]]:\n",
    "    reflection_count = state.get(\"reflection_count\", 0)\n",
    "    configurable = config.get(\"configurable\")\n",
    "    result = reflection_feedback_llm.invoke(state)\n",
    "    feedback = result.feedback\n",
    "    if (feedback == True) or (feedback.lower() == \"true\") or (reflection_count < configurable.get(\"num_reflections\")):\n",
    "        return Command(\n",
    "            update={\"reflection_feedback\": feedback},\n",
    "            goto=\"final_section_formatter\"\n",
    "        )\n",
    "    else:\n",
    "        return Command(\n",
    "            update={\"reflection_feedback\": feedback, \"reflection_count\": reflection_count + 1},\n",
    "            goto=\"query_generator\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_section_formatter_system_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(FINAL_SECTION_FORMATTER_SYSTEM_PROMPT_TEMPLATE),\n",
    "    HumanMessagePromptTemplate.from_template(template=\"Internal Knowledge: {knowledge}\\nSearch Result content: {accumulated_content}\"),\n",
    "])\n",
    "\n",
    "final_section_formatter_llm = final_section_formatter_system_prompt | llm\n",
    "\n",
    "def final_section_formatter_node(state: ResearchState, config: RunnableConfig):\n",
    "    result = final_section_formatter_llm.invoke(state)\n",
    "    return {\"final_section_content\": result.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_datagen_prompt(fields: List[SchemaField], rows: int = 10) -> str:\n",
    "    schema_instruction = {field.key: field.description for field in fields}\n",
    "\n",
    "    field_string = f\"\"\"## Response Format\n",
    "Always respond with a valid JSON array of objects:\n",
    "[\n",
    "{json.dumps(schema_instruction, indent=2)},\n",
    "// Additional entries...\n",
    "]\n",
    "\"\"\"\n",
    "    return f\"\"\"\n",
    "You are an expert Question-Answer generation assistant who has the skills of a polymath. Your task is to analyze content provided by the user and generate a comprehensive set of questions with detailed answers based on that content.\n",
    "\n",
    "## Core Instructions\n",
    "\n",
    "1. When presented with content, carefully analyze it to identify key concepts, important details, practical applications, and potential challenges or edge cases.\n",
    "\n",
    "2. Generate a diverse set of questions and answers that thoroughly cover the provided content. Your response must be in valid JSON format.\n",
    "\n",
    "3. Format code properly within JSON strings, using appropriate escape characters for special characters.\n",
    "\n",
    "4. Number of dataset rows must be {rows}\n",
    "\n",
    "{field_string}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from openai import RateLimitError, OpenAIError\n",
    "from pydantic import ValidationError\n",
    "\n",
    "def final_section_dataset_generator_node(state: ResearchState, config: RunnableConfig, max_retries: int = 3, base_wait: float = 2.0):\n",
    "    schema = state.get(\"schema\")\n",
    "    max_rows = config.get(\"configurable\").get(\"max_rows_from_each_section\")\n",
    "    FINAL_SECTION_DATASET_GENERATION_PROMPT = process_datagen_prompt(schema.generated_schema, int(max_rows))\n",
    "\n",
    "    final_section_dataset_generator_prompt = ChatPromptTemplate.from_messages([\n",
    "        SystemMessage(content=FINAL_SECTION_DATASET_GENERATION_PROMPT),\n",
    "        HumanMessagePromptTemplate.from_template(template=\"Report Structure: {report_structure}\\nSection Contents: {final_section_content}\"),\n",
    "    ])\n",
    "    final_dataset_generator_llm = final_section_dataset_generator_prompt | llm\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = final_dataset_generator_llm.invoke(state)\n",
    "            raw_text = result.content\n",
    "\n",
    "            # Clean up markdown wrapping\n",
    "            if raw_text.startswith(\"```json\"):\n",
    "                raw_text = raw_text[len(\"```json\"):].lstrip()\n",
    "            elif raw_text.startswith(\"```\"):\n",
    "                raw_text = raw_text[len(\"```\"):].lstrip()\n",
    "            if raw_text.endswith(\"```\"):\n",
    "                raw_text = raw_text[:-3].rstrip()\n",
    "\n",
    "            parsed_json = json.loads(raw_text)\n",
    "            final_package = {\"dataset\": parsed_json}\n",
    "            validated = DatasetRecords(**final_package)\n",
    "\n",
    "            return {\"final_section_dataset\": validated.dataset}\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"[JSON Parse Error] {e}\")\n",
    "            return {\"final_section_dataset\": [], \"error\": \"JSONDecodeError\"}\n",
    "\n",
    "        except ValidationError as e:\n",
    "            print(f\"[Pydantic Validation Error] {e}\")\n",
    "            return {\"final_section_dataset\": [], \"error\": \"ValidationError\"}\n",
    "\n",
    "        except RateLimitError:\n",
    "            wait_time = base_wait * (2 ** attempt)\n",
    "            print(f\"[Rate Limit] Retrying in {wait_time}s (Attempt {attempt + 1}/{max_retries})...\")\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "        except OpenAIError as e:\n",
    "            print(f\"[OpenAI Error] {e}\")\n",
    "            wait_time = base_wait * (2 ** attempt)\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Unexpected Error] {e}\")\n",
    "            return {\"final_section_dataset\": [], \"error\": str(e)}\n",
    "\n",
    "    return {\"final_section_dataset\": [], \"error\": \"Max retries exceeded\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_dataset_aggregator_node(state: AgentState, config: RunnableConfig):\n",
    "    dataset = []\n",
    "    section_datasets = state.get(\"final_section_dataset\")\n",
    "\n",
    "    for section_dataset in section_datasets:\n",
    "            dataset.append(section_dataset)\n",
    "    \n",
    "    return {\"final_dataset\": dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_builder = StateGraph(ResearchState, output=SectionOutput)\n",
    "\n",
    "research_builder.add_node(\"section_knowledge\", section_knowledge_node)\n",
    "research_builder.add_node(\"query_generator\", query_generator_node)\n",
    "research_builder.add_node(\"tavily_search\", tavily_search_node)\n",
    "research_builder.add_node(\"result_accumulator\", result_accumulator_node)\n",
    "research_builder.add_node(\"reflection\", reflection_feedback_node)\n",
    "research_builder.add_node(\"final_section_formatter\", final_section_formatter_node)\n",
    "research_builder.add_node(\"final_section_dataset_generator\", final_section_dataset_generator_node)\n",
    "\n",
    "research_builder.add_edge(START, \"section_knowledge\")\n",
    "research_builder.add_edge(\"section_knowledge\", \"query_generator\")\n",
    "research_builder.add_edge(\"query_generator\", \"tavily_search\")\n",
    "research_builder.add_edge(\"tavily_search\", \"result_accumulator\")\n",
    "research_builder.add_edge(\"result_accumulator\", \"reflection\")\n",
    "research_builder.add_edge(\"final_section_formatter\", \"final_section_dataset_generator\")\n",
    "research_builder.add_edge(\"final_section_dataset_generator\", END)\n",
    "\n",
    "memory_saver = MemorySaver()\n",
    "\n",
    "builder = StateGraph(AgentState)\n",
    "\n",
    "builder.add_node(\"schema_generator\", schema_generator_node)\n",
    "builder.add_node(\"human_feedback_on_schema\", human_feedback_on_schema_node)\n",
    "builder.add_node(\"report_structure_planner\", report_structure_planner_node)\n",
    "builder.add_node(\"human_feedback_report_structure\", human_feedback_node)\n",
    "builder.add_node(\"section_formatter\", section_formatter_node)\n",
    "builder.add_node(\"research_agent\", research_builder.compile())\n",
    "builder.add_node(\"final_dataset_aggregator\", final_dataset_aggregator_node)\n",
    "\n",
    "builder.set_entry_point(\"schema_generator\")\n",
    "builder.add_edge(\"schema_generator\", \"human_feedback_on_schema\")\n",
    "builder.add_edge(\"report_structure_planner\", \"human_feedback_report_structure\")\n",
    "builder.add_edge(\"research_agent\", \"final_dataset_aggregator\")\n",
    "builder.add_edge(\"final_dataset_aggregator\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = builder.compile(checkpointer=memory_saver)\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "TOPIC = \"Support Vector Machines\"\n",
    "OUTLINE = \"I want to have qna dataset on this topic A-Z so that the model I train would be able to answer everythin about support vector machines\"\n",
    "\n",
    "thread = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": str(uuid.uuid4()),\n",
    "        \"max_queries\": 2,\n",
    "        \"search_depth\": 1,\n",
    "        \"num_reflections\": 2,\n",
    "        \"max_rows_from_each_section\": 5\n",
    "    }\n",
    "}\n",
    "\n",
    "for event in graph.stream(\n",
    "    {\"topic\": TOPIC, \"outline\": OUTLINE},\n",
    "    config=thread,\n",
    "):\n",
    "    if \"schema_generator\" in event:\n",
    "        print(\"<<< SCHEMA GENERATOR >>>\")\n",
    "        print(event[\"schema_generator\"][\"schema\"])\n",
    "        print(\"\\n\", \"=\"*100, \"\\n\")\n",
    "    elif \"report_structure_planner\" in event:\n",
    "        print(\"<<< REPORT STRUCTURE PLANNER >>>\")\n",
    "        print(event[\"report_structure_planner\"][\"messages\"][-1].content)\n",
    "        print(\"\\n\", \"=\"*100, \"\\n\")\n",
    "    elif \"section_formatter\" in event:\n",
    "        print(\"<<< SECTION FORMATTING >>>\")\n",
    "        print(event[\"section_formatter\"])\n",
    "        print(\"\\n\", \"=\"*100, \"\\n\")\n",
    "    elif \"research_agent\" in event:\n",
    "        # check output of research_agent\n",
    "        print(\"<<< RESEARCH AGENT >>>\")\n",
    "        print(event[\"research_agent\"])\n",
    "        print(\"\\n\", \"=\"*100, \"\\n\")\n",
    "    elif \"final_dataset_aggregator\" in event:\n",
    "        # check output of final_dataset_aggregator\n",
    "        print(\"<<< FINAL REPORT WRITER >>>\")\n",
    "        print(event[\"final_dataset_aggregator\"])\n",
    "        print(\"\\n\", \"=\" * 100, \"\\n\")\n",
    "\n",
    "        output_data = event[\"final_dataset_aggregator\"]\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"final_dataset_output_{timestamp}.json\"\n",
    "        \n",
    "        output_dir = \"output_files\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        \n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(f\"Saved final dataset to: {filepath}\")\n",
    "    elif \"human_feedback_on_schema\" in event:\n",
    "        print(\"<<< HUMAN FEEDBACK ON SCHEMA >>>\")\n",
    "        print(event[\"human_feedback_on_schema\"][\"messages\"][-1].content)\n",
    "        print(\"\\n\", \"=\"*100, \"\\n\")\n",
    "    elif \"human_feedback_report_structure\" in event:\n",
    "        print(\"<<< HUMAN FEEDBACK ON REPORT STRUCTURE >>>\")\n",
    "        print(event[\"human_feedback_report_structure\"][\"messages\"][-1].content)\n",
    "        print(\"\\n\", \"=\"*100, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-research (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
